# most popular

# step func
# f(x) > 0 -> 1 else 0

# sigmoid
# f(x) = 1 / ( 1 + e^-x ) <0;1>
# zazwyczaj w ostaniej warstwie klasyfikacji binarnej

# TanH
# f(x) = (2 / ( 1 + e^-2x ))-1 <-1;1>
# ukryta warstwa

# Relu
# f(x) = max(0;x) -> 0 if x <= 0

# leaky realu
# f(x) = x if x >=0 else a*x
# ulepszone Relu, 'a' jest bardzo ma≈Çe


# softmax


# torch.nn.functional import F


